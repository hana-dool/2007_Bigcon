{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojVKajZ4SjCl"
   },
   "source": [
    "# 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1598500974334,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "qEdeA8IhSjCp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from platform import python_version\n",
    "\n",
    "# random 고정시 필요한 모듈\n",
    "import os\n",
    "import random\n",
    "\n",
    "# 모델 형성시 필요한 모듈\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "# local적인 모델 해석 \n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# random 고정\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version: 3.7.4\n",
      "numpy version : 1.18.5\n",
      "pandas version : 1.1.1\n",
      "matplotlib version : 3.3.1\n",
      "sklearn version : 0.23.2\n",
      "tensorflow version : 2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(f'python version: {python_version()}')\n",
    "print(f'numpy version : {np.__version__}')\n",
    "print(f'pandas version : {pd.__version__}')\n",
    "print(f'matplotlib version : {matplotlib.__version__}')\n",
    "print(f'sklearn version : {sklearn.__version__}')\n",
    "print(f'tensorflow version : {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gb-JDaCISjCs"
   },
   "outputs": [],
   "source": [
    "# 파일을 저장한 위치를 써 주세요.\n",
    "directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15657,
     "status": "ok",
     "timestamp": 1598506047946,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "Nvo5_FnYSjCv"
   },
   "outputs": [],
   "source": [
    "# 앞에서의 eda에 의해 형성된 xlsx 파일을 불러옵니다.\n",
    "X_df=pd.read_excel(directory+'/X_for_train.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15320,
     "status": "ok",
     "timestamp": 1598506047952,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "YYKBuU8aSjCx",
    "outputId": "b7499be7-a268-4e0c-8da8-c522daa9ce00",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>방송일시</th>\n",
       "      <th>노출(분)</th>\n",
       "      <th>마더코드</th>\n",
       "      <th>상품코드</th>\n",
       "      <th>상품명</th>\n",
       "      <th>상품군</th>\n",
       "      <th>판매단가</th>\n",
       "      <th>취급액</th>\n",
       "      <th>평균방송분</th>\n",
       "      <th>...</th>\n",
       "      <th>PrimeTime</th>\n",
       "      <th>남성상품</th>\n",
       "      <th>여성상품</th>\n",
       "      <th>무이자</th>\n",
       "      <th>일시불</th>\n",
       "      <th>유명기업/브랜드</th>\n",
       "      <th>타 채널 시청자 수 평균</th>\n",
       "      <th>가전제품</th>\n",
       "      <th>농수축소분류</th>\n",
       "      <th>어류손질여부</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01 06:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100346</td>\n",
       "      <td>201072</td>\n",
       "      <td>테이트 남성 셀린니트3종</td>\n",
       "      <td>의류</td>\n",
       "      <td>39900</td>\n",
       "      <td>2099000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 06:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100346</td>\n",
       "      <td>201079</td>\n",
       "      <td>테이트 여성 셀린니트3종</td>\n",
       "      <td>의류</td>\n",
       "      <td>39900</td>\n",
       "      <td>4371000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 06:20:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100346</td>\n",
       "      <td>201072</td>\n",
       "      <td>테이트 남성 셀린니트3종</td>\n",
       "      <td>의류</td>\n",
       "      <td>39900</td>\n",
       "      <td>3262000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-01 06:20:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100346</td>\n",
       "      <td>201079</td>\n",
       "      <td>테이트 여성 셀린니트3종</td>\n",
       "      <td>의류</td>\n",
       "      <td>39900</td>\n",
       "      <td>6955000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-01 06:40:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100346</td>\n",
       "      <td>201072</td>\n",
       "      <td>테이트 남성 셀린니트3종</td>\n",
       "      <td>의류</td>\n",
       "      <td>39900</td>\n",
       "      <td>6672000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35374</th>\n",
       "      <td>35374</td>\n",
       "      <td>2019-12-31 23:40:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100448</td>\n",
       "      <td>201391</td>\n",
       "      <td>일시불쿠첸압력밥솥 6인용</td>\n",
       "      <td>주방</td>\n",
       "      <td>148000</td>\n",
       "      <td>10157000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1335.9</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35375</th>\n",
       "      <td>35375</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100448</td>\n",
       "      <td>201383</td>\n",
       "      <td>무이자쿠첸압력밥솥 10인용</td>\n",
       "      <td>주방</td>\n",
       "      <td>178000</td>\n",
       "      <td>50929000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35376</th>\n",
       "      <td>35376</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100448</td>\n",
       "      <td>201390</td>\n",
       "      <td>일시불쿠첸압력밥솥 10인용</td>\n",
       "      <td>주방</td>\n",
       "      <td>168000</td>\n",
       "      <td>104392000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35377</th>\n",
       "      <td>35377</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100448</td>\n",
       "      <td>201384</td>\n",
       "      <td>무이자쿠첸압력밥솥 6인용</td>\n",
       "      <td>주방</td>\n",
       "      <td>158000</td>\n",
       "      <td>13765000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35378</th>\n",
       "      <td>35378</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100448</td>\n",
       "      <td>201391</td>\n",
       "      <td>일시불쿠첸압력밥솥 6인용</td>\n",
       "      <td>주방</td>\n",
       "      <td>148000</td>\n",
       "      <td>46608000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>프라임아님</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>가전제품 아님</td>\n",
       "      <td>분류에없음</td>\n",
       "      <td>해당없음</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35379 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                 방송일시  노출(분)    마더코드    상품코드             상품명  \\\n",
       "0               0  2019-01-01 06:00:00   20.0  100346  201072   테이트 남성 셀린니트3종   \n",
       "1               1  2019-01-01 06:00:00   20.0  100346  201079   테이트 여성 셀린니트3종   \n",
       "2               2  2019-01-01 06:20:00   20.0  100346  201072   테이트 남성 셀린니트3종   \n",
       "3               3  2019-01-01 06:20:00   20.0  100346  201079   테이트 여성 셀린니트3종   \n",
       "4               4  2019-01-01 06:40:00   20.0  100346  201072   테이트 남성 셀린니트3종   \n",
       "...           ...                  ...    ...     ...     ...             ...   \n",
       "35374       35374  2019-12-31 23:40:00   20.0  100448  201391   일시불쿠첸압력밥솥 6인용   \n",
       "35375       35375  2020-01-01 00:00:00   20.0  100448  201383  무이자쿠첸압력밥솥 10인용   \n",
       "35376       35376  2020-01-01 00:00:00   20.0  100448  201390  일시불쿠첸압력밥솥 10인용   \n",
       "35377       35377  2020-01-01 00:00:00   20.0  100448  201384   무이자쿠첸압력밥솥 6인용   \n",
       "35378       35378  2020-01-01 00:00:00   20.0  100448  201391   일시불쿠첸압력밥솥 6인용   \n",
       "\n",
       "      상품군    판매단가        취급액  평균방송분  ...  PrimeTime 남성상품  여성상품  무이자  일시불  \\\n",
       "0      의류   39900    2099000   10.0  ...      프라임아님    1     0    0    0   \n",
       "1      의류   39900    4371000   10.0  ...      프라임아님    0     1    0    0   \n",
       "2      의류   39900    3262000   10.0  ...      프라임아님    1     0    0    0   \n",
       "3      의류   39900    6955000   10.0  ...      프라임아님    0     1    0    0   \n",
       "4      의류   39900    6672000   10.0  ...      프라임아님    1     0    0    0   \n",
       "...    ..     ...        ...    ...  ...        ...  ...   ...  ...  ...   \n",
       "35374  주방  148000   10157000    5.0  ...      프라임아님    0     0    0    1   \n",
       "35375  주방  178000   50929000    5.0  ...      프라임아님    0     0    1    0   \n",
       "35376  주방  168000  104392000    5.0  ...      프라임아님    0     0    0    1   \n",
       "35377  주방  158000   13765000    5.0  ...      프라임아님    0     0    1    0   \n",
       "35378  주방  148000   46608000    5.0  ...      프라임아님    0     0    0    1   \n",
       "\n",
       "       유명기업/브랜드 타 채널 시청자 수 평균     가전제품  농수축소분류  어류손질여부  \n",
       "0             0        1520.0  가전제품 아님   분류에없음    해당없음  \n",
       "1             0        1520.0  가전제품 아님   분류에없음    해당없음  \n",
       "2             0        1520.0  가전제품 아님   분류에없음    해당없음  \n",
       "3             0        1520.0  가전제품 아님   분류에없음    해당없음  \n",
       "4             0        1520.0  가전제품 아님   분류에없음    해당없음  \n",
       "...         ...           ...      ...     ...     ...  \n",
       "35374         1        1335.9  가전제품 아님   분류에없음    해당없음  \n",
       "35375         1        1520.0  가전제품 아님   분류에없음    해당없음  \n",
       "35376         1        1520.0  가전제품 아님   분류에없음    해당없음  \n",
       "35377         1        1520.0  가전제품 아님   분류에없음    해당없음  \n",
       "35378         1        1520.0  가전제품 아님   분류에없음    해당없음  \n",
       "\n",
       "[35379 rows x 55 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_df.copy() ; X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target data 를 빼냅니다.\n",
    "y=X['취급액']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9981,
     "status": "ok",
     "timestamp": 1598506047956,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "_FRK1wnXSjC9"
   },
   "outputs": [],
   "source": [
    "# 예측에 쓰지 않는 열 제거\n",
    "X.drop(columns = ['Unnamed: 0','취급액','방송일시','마더코드','상품코드','상품명','년','일','시분','월일','시간열','정수노출(분)','요일/시간','날짜','시각'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1598506052648,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "c5HOi8iySjDF"
   },
   "outputs": [],
   "source": [
    "# 더미화\n",
    "X = pd.get_dummies(X,columns=['월','요일','분기','상품군','PrimeTime','어류손질여부','가전제품','농수축소분류'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1598506073148,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "qCAuwTZ6Yr--"
   },
   "outputs": [],
   "source": [
    "# X_col 의 이름 저장\n",
    "X_features=X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6652,
     "status": "ok",
     "timestamp": 1598500996513,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "InxVyYLCSjDJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10967928, -0.55549003, -0.20858117, ..., -0.06437268,\n",
       "        -0.05297285, -0.08298992],\n",
       "       [-0.10967928, -0.55549003, -0.20858117, ..., -0.06437268,\n",
       "        -0.05297285, -0.08298992],\n",
       "       [-0.10967928, -0.55549003, -0.20858117, ..., -0.06437268,\n",
       "        -0.05297285, -0.08298992],\n",
       "       ...,\n",
       "       [-0.10967928, -0.35098812, -0.92228108, ..., -0.06437268,\n",
       "        -0.05297285, -0.08298992],\n",
       "       [-0.10967928, -0.36695236, -0.92228108, ..., -0.06437268,\n",
       "        -0.05297285, -0.08298992],\n",
       "       [-0.10967928, -0.3829166 , -0.92228108, ..., -0.06437268,\n",
       "        -0.05297285, -0.08298992]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling 과정\n",
    "# 큰 skewness 를 가지는 값들은 대부분 애초에 분포가 너무 틀어져있어 (노출 분거의 다 20,10분의 값을 가진다.) log/squre 등의 변환을 해도 똑같을거라 판단하고, scaling 만 하기로 하였다.\n",
    "# 그리고, categorical 데이터의 경우에도 scaling 을 해 주어서 평균 0 / 분산 1 을 맞추어주는게 나중에 DNN 이 학습을 더 잘할 거라 판단하였다.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:25:37.637397Z",
     "start_time": "2020-08-02T09:25:37.607478Z"
    },
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4154,
     "status": "ok",
     "timestamp": 1598500996786,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "emZ1Hh6ISjDR"
   },
   "outputs": [],
   "source": [
    "# dataset train/test set 으로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 의 train 을 valid / train 으로 다시 나누기\n",
    "X_train_, X_valid_, y_train_, y_valid_ = train_test_split(X_train, y_train, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3568,
     "status": "ok",
     "timestamp": 1598500996787,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "jf1r445TSjDT"
   },
   "outputs": [],
   "source": [
    "# MAPE 정의\n",
    "def MAPE(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RBcn3TteSjDU"
   },
   "source": [
    "# DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 137902,
     "status": "ok",
     "timestamp": 1598465732490,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "hUPJG_KeSjDf",
    "outputId": "b7f6aa34-ec35-4c95-e824-dc50a554c1d7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 67.0023 - val_loss: 50.5020\n",
      "Epoch 2/160\n",
      "708/708 [==============================] - 12s 18ms/step - loss: 48.1414 - val_loss: 47.6521\n",
      "Epoch 3/160\n",
      "708/708 [==============================] - 12s 17ms/step - loss: 46.2831 - val_loss: 47.7743\n",
      "Epoch 4/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 45.0542 - val_loss: 45.7287\n",
      "Epoch 5/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 43.9036 - val_loss: 45.4058\n",
      "Epoch 6/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 43.2272 - val_loss: 45.0606\n",
      "Epoch 7/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 42.8870 - val_loss: 45.5952\n",
      "Epoch 8/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 42.2005 - val_loss: 43.7205\n",
      "Epoch 9/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 42.0198 - val_loss: 44.0484\n",
      "Epoch 10/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 41.7046 - val_loss: 43.6438\n",
      "Epoch 11/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 41.4676 - val_loss: 43.8717\n",
      "Epoch 12/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 41.0219 - val_loss: 43.2790\n",
      "Epoch 13/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 40.7976 - val_loss: 45.2119\n",
      "Epoch 14/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 40.8999 - val_loss: 42.9991\n",
      "Epoch 15/160\n",
      "708/708 [==============================] - 16s 23ms/step - loss: 40.4622 - val_loss: 42.9689\n",
      "Epoch 16/160\n",
      "708/708 [==============================] - 16s 23ms/step - loss: 40.3744 - val_loss: 42.9399\n",
      "Epoch 17/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 40.0416 - val_loss: 42.7554\n",
      "Epoch 18/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 39.8688 - val_loss: 42.4864\n",
      "Epoch 19/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 39.9924 - val_loss: 43.0638\n",
      "Epoch 20/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 39.7282 - val_loss: 42.6548\n",
      "Epoch 21/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 39.6383 - val_loss: 43.7045\n",
      "Epoch 22/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 39.3986 - val_loss: 43.2633\n",
      "Epoch 23/160\n",
      "708/708 [==============================] - 16s 23ms/step - loss: 39.5730 - val_loss: 42.3503\n",
      "Epoch 24/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 39.2737 - val_loss: 42.4120\n",
      "Epoch 25/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 39.0449 - val_loss: 42.7803\n",
      "Epoch 26/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 38.9467 - val_loss: 42.4889\n",
      "Epoch 27/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 38.8120 - val_loss: 41.8721\n",
      "Epoch 28/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 38.7180 - val_loss: 41.9961\n",
      "Epoch 29/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 38.8458 - val_loss: 42.2483\n",
      "Epoch 30/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 38.5639 - val_loss: 42.0383\n",
      "Epoch 31/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 38.6773 - val_loss: 41.8180\n",
      "Epoch 32/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 38.4065 - val_loss: 42.9616\n",
      "Epoch 33/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 38.4120 - val_loss: 42.2705\n",
      "Epoch 34/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 38.2443 - val_loss: 42.0169\n",
      "Epoch 35/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 38.0923 - val_loss: 42.2234\n",
      "Epoch 36/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 38.0553 - val_loss: 42.2994\n",
      "Epoch 37/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 38.0235 - val_loss: 41.9918\n",
      "Epoch 38/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 37.8179 - val_loss: 41.9158\n",
      "Epoch 39/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 37.8842 - val_loss: 41.8261\n",
      "Epoch 40/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 37.8824 - val_loss: 41.6536\n",
      "Epoch 41/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 37.5744 - val_loss: 41.7836\n",
      "Epoch 42/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 37.7443 - val_loss: 43.1921\n",
      "Epoch 43/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 37.8049 - val_loss: 41.7174\n",
      "Epoch 44/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 37.4318 - val_loss: 42.1323\n",
      "Epoch 45/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 37.4271 - val_loss: 41.6200\n",
      "Epoch 46/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 37.2598 - val_loss: 42.4819\n",
      "Epoch 47/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 37.0687 - val_loss: 41.8561\n",
      "Epoch 48/160\n",
      "708/708 [==============================] - 16s 23ms/step - loss: 37.3458 - val_loss: 41.5028\n",
      "Epoch 49/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 37.0778 - val_loss: 42.0739\n",
      "Epoch 50/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 37.1002 - val_loss: 41.6360\n",
      "Epoch 51/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 37.0986 - val_loss: 41.9386\n",
      "Epoch 52/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 36.9683 - val_loss: 42.0897\n",
      "Epoch 53/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 37.0477 - val_loss: 41.4890\n",
      "Epoch 54/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 36.8780 - val_loss: 41.5190\n",
      "Epoch 55/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 36.8719 - val_loss: 41.3571\n",
      "Epoch 56/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 36.7598 - val_loss: 41.6375\n",
      "Epoch 57/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 36.7206 - val_loss: 42.0179\n",
      "Epoch 58/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 36.4831 - val_loss: 41.5507\n",
      "Epoch 59/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 36.3737 - val_loss: 41.1706\n",
      "Epoch 60/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 36.5261 - val_loss: 41.4141\n",
      "Epoch 61/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 36.7843 - val_loss: 41.7180\n",
      "Epoch 62/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 36.4914 - val_loss: 41.3356\n",
      "Epoch 63/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 36.3961 - val_loss: 42.0118\n",
      "Epoch 64/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 36.6337 - val_loss: 41.1953\n",
      "Epoch 65/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 36.3530 - val_loss: 41.3738\n",
      "Epoch 66/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 36.2019 - val_loss: 41.5684\n",
      "Epoch 67/160\n",
      "708/708 [==============================] - 16s 23ms/step - loss: 36.4036 - val_loss: 41.6823\n",
      "Epoch 68/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 36.1535 - val_loss: 41.2204\n",
      "Epoch 69/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 36.1374 - val_loss: 41.4020\n",
      "Epoch 70/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 36.0540 - val_loss: 41.2393\n",
      "Epoch 71/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 36.0039 - val_loss: 41.2616\n",
      "Epoch 72/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 35.9112 - val_loss: 41.7808\n",
      "Epoch 73/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 35.9718 - val_loss: 41.2315\n",
      "Epoch 74/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 36.0719 - val_loss: 41.9461\n",
      "Epoch 75/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 35.9295 - val_loss: 41.6388\n",
      "222/222 [==============================] - 1s 4ms/step - loss: 41.0262\n"
     ]
    }
   ],
   "source": [
    "# random 고정\n",
    "tf.random.set_seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=1, \n",
    "    inter_op_parallelism_threads=1)\n",
    "\n",
    "sess = tf.compat.v1.Session(\n",
    "    graph=tf.compat.v1.get_default_graph(), \n",
    "    config=session_conf)\n",
    "\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# 모델 훈련\n",
    "model1 = keras.models.Sequential()\n",
    "model1 = Sequential([\n",
    "    Dense(128, kernel_initializer='normal', activation = \"relu\", input_shape=X_train.shape[1:]), \n",
    "    Dropout(0.2),\n",
    "    Dense(256, kernel_initializer='normal', activation = \"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, kernel_initializer='normal', activation = \"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(1024, kernel_initializer='normal', activation = \"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, kernel_initializer='normal', activation = \"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, kernel_initializer='normal', activation = \"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, kernel_initializer='normal', activation = \"relu\"),\n",
    "    Dense(1, kernel_initializer='normal'), ])\n",
    "\n",
    "model1.compile(loss=\"mape\",  # 평가기준이 mape 이니까 이걸로 하자.\n",
    "              optimizer=keras.optimizers.Adam())\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"model1.h5\", #저장할 모델 이름\n",
    "                                             monitor = 'val_loss', #monitoring 할 기준\n",
    "                                             save_best_only=True ) # \n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=16, #2 만 줘보자.\n",
    "                                             restore_best_weights=True)\n",
    "history1 = model1.fit(X_train_, y_train_, \n",
    "                        epochs=160,\n",
    "                        validation_data=(X_valid_, y_valid_),\n",
    "                        callbacks=[checkpoint,early_stopping])\n",
    "model1 = keras.models.load_model(filepath = \"model1.h5\")\n",
    "evaluation1 = model1.evaluate(X_test, y_test) \n",
    "# validation 에 대해 41.0262 의 값이 나왔다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 104976,
     "status": "ok",
     "timestamp": 1598466003147,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "sk7JdzcKSjDh",
    "outputId": "6d989a71-b5a8-4cc2-b5bf-09df0d54345b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 69.4502 - val_loss: 53.3374\n",
      "Epoch 2/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 52.6136 - val_loss: 51.6379\n",
      "Epoch 3/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 50.2929 - val_loss: 49.4820\n",
      "Epoch 4/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 48.8782 - val_loss: 46.6362\n",
      "Epoch 5/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 47.3684 - val_loss: 46.5361\n",
      "Epoch 6/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 47.0321 - val_loss: 46.1121\n",
      "Epoch 7/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 46.0544 - val_loss: 45.8611\n",
      "Epoch 8/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 45.4893 - val_loss: 44.6826\n",
      "Epoch 9/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 45.2201 - val_loss: 44.3587\n",
      "Epoch 10/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 44.6814 - val_loss: 44.1177\n",
      "Epoch 11/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 44.7815 - val_loss: 44.6011\n",
      "Epoch 12/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 44.2185 - val_loss: 43.7952\n",
      "Epoch 13/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 43.9321 - val_loss: 43.8875\n",
      "Epoch 14/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 43.5533 - val_loss: 43.2215\n",
      "Epoch 15/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 43.3988 - val_loss: 42.7886\n",
      "Epoch 16/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 43.1449 - val_loss: 43.1935\n",
      "Epoch 17/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 43.0457 - val_loss: 42.7081\n",
      "Epoch 18/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 42.8837 - val_loss: 43.0413\n",
      "Epoch 19/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 42.6204 - val_loss: 42.7017\n",
      "Epoch 20/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 42.6212 - val_loss: 42.6052\n",
      "Epoch 21/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 42.1812 - val_loss: 43.2662\n",
      "Epoch 22/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 42.2474 - val_loss: 41.8944\n",
      "Epoch 23/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 42.0107 - val_loss: 41.8918\n",
      "Epoch 24/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 41.7673 - val_loss: 42.0302\n",
      "Epoch 25/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 41.5170 - val_loss: 41.8059\n",
      "Epoch 26/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 41.2415 - val_loss: 42.3801\n",
      "Epoch 27/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 41.5568 - val_loss: 42.2592\n",
      "Epoch 28/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 41.2168 - val_loss: 42.6858\n",
      "Epoch 29/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 41.0463 - val_loss: 41.7706\n",
      "Epoch 30/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.8443 - val_loss: 41.9693\n",
      "Epoch 31/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.6729 - val_loss: 41.4465\n",
      "Epoch 32/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.7193 - val_loss: 41.7673\n",
      "Epoch 33/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.8031 - val_loss: 41.3905\n",
      "Epoch 34/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.5124 - val_loss: 41.5422\n",
      "Epoch 35/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.4915 - val_loss: 42.2813\n",
      "Epoch 36/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.1848 - val_loss: 41.8621\n",
      "Epoch 37/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.0808 - val_loss: 42.1927\n",
      "Epoch 38/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.3379 - val_loss: 40.9794\n",
      "Epoch 39/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 40.2673 - val_loss: 41.6749\n",
      "Epoch 40/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.8724 - val_loss: 41.4635\n",
      "Epoch 41/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.7635 - val_loss: 41.8288\n",
      "Epoch 42/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.7916 - val_loss: 42.0947\n",
      "Epoch 43/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.7800 - val_loss: 41.0018\n",
      "Epoch 44/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.2927 - val_loss: 41.1473\n",
      "Epoch 45/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.7394 - val_loss: 41.1491\n",
      "Epoch 46/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.6695 - val_loss: 41.2534\n",
      "Epoch 47/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.3780 - val_loss: 41.5247\n",
      "Epoch 48/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.4981 - val_loss: 42.0557\n",
      "Epoch 49/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.3983 - val_loss: 41.2598\n",
      "Epoch 50/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.2726 - val_loss: 41.5547\n",
      "Epoch 51/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.1367 - val_loss: 41.5389\n",
      "Epoch 52/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 39.2515 - val_loss: 41.4746\n",
      "Epoch 53/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 38.9404 - val_loss: 41.0327\n",
      "Epoch 54/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 38.9549 - val_loss: 41.3322\n",
      "Epoch 55/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 38.8873 - val_loss: 41.6261\n",
      "Epoch 56/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 38.7436 - val_loss: 41.0922\n",
      "Epoch 57/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 38.9134 - val_loss: 41.5623\n",
      "Epoch 58/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 38.7647 - val_loss: 41.5681\n",
      "Epoch 59/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 38.6853 - val_loss: 41.3448\n",
      "Epoch 60/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 38.7281 - val_loss: 41.1477\n",
      "Epoch 61/160\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 38.4457 - val_loss: 41.3743\n",
      "Epoch 62/160\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 38.4759 - val_loss: 40.9993\n",
      "222/222 [==============================] - 0s 1ms/step - loss: 41.4471\n"
     ]
    }
   ],
   "source": [
    "# random 고정\n",
    "tf.random.set_seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=1, \n",
    "    inter_op_parallelism_threads=1)\n",
    "\n",
    "sess = tf.compat.v1.Session(\n",
    "    graph=tf.compat.v1.get_default_graph(), \n",
    "    config=session_conf)\n",
    "\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# 모델 훈련\n",
    "model2 = keras.models.Sequential()\n",
    "model2.add(keras.layers.Dense(200, activation=\"selu\",kernel_initializer=\"lecun_normal\",input_shape=X_train.shape[1:]))\n",
    "model2.add(Dropout(0.2))\n",
    "for layer in range(5):\n",
    "    model2.add(keras.layers.Dense(200, activation=\"selu\",kernel_initializer=\"lecun_normal\"))\n",
    "    model2.add(Dropout(0.2))\n",
    "model2.add(keras.layers.Dense(100, activation=\"selu\",kernel_initializer=\"lecun_normal\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(keras.layers.Dense(10))\n",
    "model2.add(keras.layers.Dense(1))\n",
    "model2.compile(loss=\"mape\",  # 평가기준이 mape 이니까 이걸로 하자.\n",
    "              optimizer=keras.optimizers.Adam())\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"model2.h5\", #저장할 모델 이름\n",
    "                                             monitor = 'val_loss', #monitoring 할 기준\n",
    "                                             save_best_only=True ) # \n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=24, #2 만 줘보자.\n",
    "                                             restore_best_weights=True)\n",
    "\n",
    "history2 = model2.fit(X_train_, y_train_, \n",
    "                        epochs=160,\n",
    "                        validation_data=(X_valid_, y_valid_),\n",
    "                        callbacks=[checkpoint,early_stopping])\n",
    "model2 = keras.models.load_model(filepath = \"model2.h5\")\n",
    "evaluation2 = model2.evaluate(X_test, y_test) \n",
    "# validation set 에 대해 mape 가 41.4471 이 나왔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1502634,
     "status": "ok",
     "timestamp": 1598504698825,
     "user": {
      "displayName": "rany go",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64",
      "userId": "15846384859939162308"
     },
     "user_tz": -540
    },
    "id": "lcAABSovFeeb",
    "outputId": "22245092-169b-4069-c69d-a6db8047c6ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "708/708 [==============================] - 11s 15ms/step - loss: 62.1580 - val_loss: 51.8562\n",
      "Epoch 2/160\n",
      "708/708 [==============================] - 11s 16ms/step - loss: 51.2635 - val_loss: 48.1708\n",
      "Epoch 3/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 49.6192 - val_loss: 47.6704\n",
      "Epoch 4/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 48.1465 - val_loss: 46.1541\n",
      "Epoch 5/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 46.9414 - val_loss: 46.7387\n",
      "Epoch 6/160\n",
      "708/708 [==============================] - 16s 23ms/step - loss: 45.9123 - val_loss: 45.4564\n",
      "Epoch 7/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 45.5861 - val_loss: 46.7668\n",
      "Epoch 8/160\n",
      "708/708 [==============================] - 17s 24ms/step - loss: 44.8474 - val_loss: 43.6313\n",
      "Epoch 9/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 44.5158 - val_loss: 44.4806\n",
      "Epoch 10/160\n",
      "708/708 [==============================] - 17s 24ms/step - loss: 44.1077 - val_loss: 43.8669\n",
      "Epoch 11/160\n",
      "708/708 [==============================] - 17s 24ms/step - loss: 44.3779 - val_loss: 44.3173\n",
      "Epoch 12/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 43.4823 - val_loss: 42.8783\n",
      "Epoch 13/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 42.8775 - val_loss: 44.6573\n",
      "Epoch 14/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 43.1554 - val_loss: 43.0409\n",
      "Epoch 15/160\n",
      "708/708 [==============================] - 17s 24ms/step - loss: 42.6798 - val_loss: 42.0485\n",
      "Epoch 16/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 42.3268 - val_loss: 42.2993\n",
      "Epoch 17/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 42.3157 - val_loss: 43.6850\n",
      "Epoch 18/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 41.9781 - val_loss: 42.3461\n",
      "Epoch 19/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 42.2034 - val_loss: 41.9969\n",
      "Epoch 20/160\n",
      "708/708 [==============================] - 16s 23ms/step - loss: 41.7518 - val_loss: 42.3423\n",
      "Epoch 21/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 41.6996 - val_loss: 42.2695\n",
      "Epoch 22/160\n",
      "708/708 [==============================] - 17s 24ms/step - loss: 41.1009 - val_loss: 41.7588\n",
      "Epoch 23/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 41.2353 - val_loss: 41.4205\n",
      "Epoch 24/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 41.3309 - val_loss: 41.3599\n",
      "Epoch 25/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 41.1084 - val_loss: 41.8819\n",
      "Epoch 26/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 40.9858 - val_loss: 42.0958\n",
      "Epoch 27/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 40.6986 - val_loss: 41.1628\n",
      "Epoch 28/160\n",
      "708/708 [==============================] - 15s 22ms/step - loss: 40.3097 - val_loss: 41.4691\n",
      "Epoch 29/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 40.0034 - val_loss: 41.6822\n",
      "Epoch 30/160\n",
      "708/708 [==============================] - 15s 21ms/step - loss: 40.4345 - val_loss: 41.2324\n",
      "Epoch 31/160\n",
      "708/708 [==============================] - 16s 22ms/step - loss: 40.0445 - val_loss: 41.5840\n",
      "Epoch 32/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 40.0540 - val_loss: 42.2897\n",
      "Epoch 33/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 40.1026 - val_loss: 42.1087\n",
      "Epoch 34/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 39.6597 - val_loss: 41.3344\n",
      "Epoch 35/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 39.7579 - val_loss: 43.0196\n",
      "Epoch 36/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 39.7631 - val_loss: 41.6148\n",
      "Epoch 37/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 39.5586 - val_loss: 42.0963\n",
      "Epoch 38/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 39.3955 - val_loss: 41.1256\n",
      "Epoch 39/160\n",
      "708/708 [==============================] - 12s 18ms/step - loss: 39.2786 - val_loss: 41.4486\n",
      "Epoch 40/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 39.1896 - val_loss: 41.9333\n",
      "Epoch 41/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 39.0913 - val_loss: 41.2295\n",
      "Epoch 42/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 39.1209 - val_loss: 42.1675\n",
      "Epoch 43/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 39.3039 - val_loss: 41.6277\n",
      "Epoch 44/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 38.9539 - val_loss: 41.0003\n",
      "Epoch 45/160\n",
      "708/708 [==============================] - 14s 20ms/step - loss: 38.7975 - val_loss: 41.2363\n",
      "Epoch 46/160\n",
      "708/708 [==============================] - 14s 19ms/step - loss: 38.7303 - val_loss: 40.8251\n",
      "Epoch 47/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 38.7756 - val_loss: 41.0117\n",
      "Epoch 48/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 38.4671 - val_loss: 40.7550\n",
      "Epoch 49/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 38.4791 - val_loss: 41.1819\n",
      "Epoch 50/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 38.3720 - val_loss: 41.4079\n",
      "Epoch 51/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 38.3583 - val_loss: 41.4886\n",
      "Epoch 52/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 38.4917 - val_loss: 40.6748\n",
      "Epoch 53/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 37.9869 - val_loss: 40.5583\n",
      "Epoch 54/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 38.0941 - val_loss: 41.0561\n",
      "Epoch 55/160\n",
      "708/708 [==============================] - 12s 18ms/step - loss: 37.9567 - val_loss: 40.3436\n",
      "Epoch 56/160\n",
      "708/708 [==============================] - 12s 17ms/step - loss: 38.0975 - val_loss: 41.3045\n",
      "Epoch 57/160\n",
      "708/708 [==============================] - 12s 18ms/step - loss: 37.8354 - val_loss: 44.2221\n",
      "Epoch 58/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 37.8590 - val_loss: 41.3682\n",
      "Epoch 59/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 37.7963 - val_loss: 41.3378\n",
      "Epoch 60/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 37.9422 - val_loss: 41.1254\n",
      "Epoch 61/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 38.1243 - val_loss: 41.4243\n",
      "Epoch 62/160\n",
      "708/708 [==============================] - 13s 19ms/step - loss: 37.6567 - val_loss: 41.1523\n",
      "Epoch 63/160\n",
      "708/708 [==============================] - 12s 18ms/step - loss: 37.6317 - val_loss: 41.1319\n",
      "Epoch 64/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 37.4914 - val_loss: 41.7516\n",
      "Epoch 65/160\n",
      "708/708 [==============================] - 13s 18ms/step - loss: 37.5163 - val_loss: 42.1788\n",
      "Epoch 66/160\n",
      "708/708 [==============================] - 10s 14ms/step - loss: 37.6199 - val_loss: 42.1617\n",
      "Epoch 67/160\n",
      "708/708 [==============================] - 9s 13ms/step - loss: 37.3892 - val_loss: 40.8523\n",
      "Epoch 68/160\n",
      "708/708 [==============================] - 9s 13ms/step - loss: 37.6884 - val_loss: 40.5662\n",
      "Epoch 69/160\n",
      "708/708 [==============================] - 12s 17ms/step - loss: 37.2341 - val_loss: 40.9412\n",
      "Epoch 70/160\n",
      "708/708 [==============================] - 12s 18ms/step - loss: 37.1783 - val_loss: 41.2876\n",
      "Epoch 71/160\n",
      "708/708 [==============================] - 12s 17ms/step - loss: 37.2735 - val_loss: 41.6216\n",
      "222/222 [==============================] - 1s 4ms/step - loss: 40.6775\n"
     ]
    }
   ],
   "source": [
    "# random 고정\n",
    "tf.random.set_seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=1, \n",
    "    inter_op_parallelism_threads=1)\n",
    "\n",
    "sess = tf.compat.v1.Session(\n",
    "    graph=tf.compat.v1.get_default_graph(), \n",
    "    config=session_conf)\n",
    "\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "model3 = keras.models.Sequential()\n",
    "model3 = Sequential([\n",
    "    Dense(128, kernel_initializer='he_normal', activation = \"elu\", input_shape=X_train.shape[1:]),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, kernel_initializer='he_normal', activation = \"elu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, kernel_initializer='he_normal', activation = \"elu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, kernel_initializer='he_normal', activation = \"elu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, kernel_initializer='he_normal', activation = \"elu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, kernel_initializer='he_normal', activation = \"elu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, kernel_initializer='he_normal', activation = \"elu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, kernel_initializer='he_normal', activation = \"elu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, kernel_initializer='he_normal', activation = \"elu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, kernel_initializer='he_normal'), \n",
    "])\n",
    "\n",
    "model3.compile(loss=\"mape\",  # 평가기준이 mape 이니까 이걸로 하자.\n",
    "              optimizer=keras.optimizers.Adam())\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"model3.h5\", #저장할 모델 이름\n",
    "                                             monitor = 'val_loss', #monitoring 할 기준\n",
    "                                             save_best_only=True ) # \n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=16, #2 만 줘보자.\n",
    "                                             restore_best_weights=True)\n",
    "\n",
    "history3 = model3.fit(X_train_, y_train_, \n",
    "                        epochs=160,\n",
    "                        validation_data=(X_valid_, y_valid_),\n",
    "                        callbacks=[checkpoint,early_stopping])\n",
    "model3 = keras.models.load_model(filepath = \"model3.h5\")\n",
    "evaluation3 = model3.evaluate(X_test, y_test)\n",
    "# validation set 에 대해 mape 가 40.6775 가 나왔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4uWY4aybSjDp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.11364716950374"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3개의 모델을 각 가중치를 주어서, 합쳤습니다.\n",
    "# 40.1136 의 mape 가 나왔음.\n",
    "y_pred = 0.3*model1.predict(X_test)+ 0.35*model2.predict(X_test)+ 0.35*model3.predict(X_test)\n",
    "MAPE(y_test,y_pred.reshape(-1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "케라스 모델링.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.056px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
